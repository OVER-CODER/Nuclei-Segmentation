{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmoBz2koB4y8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, color\n",
        "from skimage.transform import resize\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "from stardist import models, data\n",
        "from stardist.models import Config2D\n",
        "from stardist.plot import render_label, render_label_pred\n",
        "from stardist.plot import random_label_cmap, draw_polygons\n",
        "from csbdeep.utils import normalize\n",
        "from stardist.matching import matching\n",
        "from skimage.measure import label as sklabel\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Processing"
      ],
      "metadata": {
        "id": "5dAn4_YJCArw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NucleiDatasetStardist(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, image_size=(256, 256)):\n",
        "        self.image_size = image_size\n",
        "        self.image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)])\n",
        "        self.mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir)])\n",
        "        assert len(self.image_files) == len(self.mask_files), \"Number of images and masks should be the same.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        mask_path = self.mask_files[idx]\n",
        "\n",
        "        img = io.imread(img_path)\n",
        "        mask = io.imread(mask_path)\n",
        "\n",
        "        if img.ndim == 3 and img.shape[2] == 4:\n",
        "            img = img[:, :, :3]\n",
        "        if img.ndim == 3:\n",
        "            img = color.rgb2gray(img)\n",
        "        img_resized = resize(img, self.image_size, anti_aliasing=True).astype(np.float32)\n",
        "        img_expanded = np.expand_dims(img_resized, axis=-1) # Add channel dimension\n",
        "\n",
        "        if mask.ndim == 3:\n",
        "            mask = color.rgb2gray(mask)\n",
        "        mask_resized = resize(mask, self.image_size, anti_aliasing=False, order=0, preserve_range=True)\n",
        "        mask_binary = mask_resized > 0.5\n",
        "        mask_instance = sklabel(mask_binary).astype(np.uint16)\n",
        "\n",
        "        return img_expanded, mask_instance"
      ],
      "metadata": {
        "id": "EnoknrbwCCLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training StarDist"
      ],
      "metadata": {
        "id": "8p_P9z0vCFhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stardist(train_loader, val_loader, model_name='stardist_nuclei', n_rays=32, epochs=10, learning_rate=1e-4):\n",
        "    config = Config2D(\n",
        "        n_rays=n_rays,\n",
        "        n_channel_in=1,  # Assuming grayscale input images\n",
        "        train_epochs=epochs,\n",
        "        train_learning_rate=learning_rate,\n",
        "        train_batch_size=4, # Match your DataLoader batch size\n",
        "        # Add other configuration parameters as needed\n",
        "    )\n",
        "\n",
        "    model = models.StarDist2D(config, name=model_name)\n",
        "\n",
        "    X_train = np.array([item[0] for item in train_loader.dataset])\n",
        "    Y_train = np.array([item[1] for item in train_loader.dataset])\n",
        "    X_val = np.array([item[0] for item in val_loader.dataset])\n",
        "    Y_val = np.array([item[1] for item in val_loader.dataset])\n",
        "\n",
        "    # Normalize images\n",
        "    X_train = [normalize(x, 0, 1) for x in X_train]\n",
        "    X_val = [normalize(x, 0, 1) for x in X_val]\n",
        "\n",
        "    model.train(X_train, Y_train, validation_data=(X_val, Y_val),\n",
        "                epochs=epochs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GhPuKyA-CI-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction with StarDist"
      ],
      "metadata": {
        "id": "acyf_fxJCMmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_stardist(model, test_loader, prob_threshold=0.5, nms_threshold=0.4):\n",
        "    predictions = []\n",
        "    for img, _ in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        img_np = img.squeeze().numpy()\n",
        "        img_norm = normalize(img_np, 0, 1)\n",
        "        # Pass the custom thresholds to predict_instances\n",
        "        labels, details = model.predict_instances(img_norm, prob_thresh=prob_threshold, nms_thresh=nms_threshold)\n",
        "        predictions.append(labels)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "82_U0x-dCRTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation (Instance Segmentation)"
      ],
      "metadata": {
        "id": "cCnnMlN8CT0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_stardist(ground_truth_masks, predictions, iou_threshold=0.5):\n",
        "    mean_iou = 0\n",
        "    total_tp = 0\n",
        "    total_fp = 0\n",
        "    total_fn = 0\n",
        "    num_samples = len(ground_truth_masks)\n",
        "\n",
        "    for gt_inst, pred_inst in zip(ground_truth_masks, predictions):\n",
        "        if gt_inst.max() > 0 or pred_inst.max() > 0:\n",
        "            stats = matching(gt_inst, pred_inst, thresh=iou_threshold)\n",
        "            if stats is not None:\n",
        "                mean_iou += stats.mean_iou if hasattr(stats, 'mean_iou') else stats.iou.mean() if hasattr(stats, 'iou') and len(stats.iou) > 0 else 0\n",
        "\n",
        "                tp, fp, fn = stats.tp, stats.fp, stats.fn\n",
        "                total_tp += tp\n",
        "                total_fp += fp\n",
        "                total_fn += fn\n",
        "\n",
        "    mean_iou /= num_samples if num_samples > 0 else 0\n",
        "\n",
        "    precision = total_tp / (total_tp + total_fp + 1e-8)\n",
        "    recall = total_tp / (total_tp + total_fn + 1e-8)\n",
        "    f1_score = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "    # This pixel-wise accuracy is likely not meaningful for instance segmentation\n",
        "    accuracy = (total_tp + np.sum((np.array(ground_truth_masks) == 0) & (np.array(predictions) == 0))) / (np.array(ground_truth_masks).size + 1e-8)\n",
        "\n",
        "    return mean_iou, precision, recall, f1_score, accuracy"
      ],
      "metadata": {
        "id": "CBlpbSZ9CXk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "Qvs3bOYpCbQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_stardist_results(images, ground_truth_masks, predictions, num_samples=5, save_dir='results/stardist'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
        "        plt.title('Original Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(ground_truth_masks[i], cmap='nipy_spectral')\n",
        "        plt.title('Ground Truth (Instance)')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(predictions[i], cmap='nipy_spectral')\n",
        "        plt.title('StarDist Prediction (Instance)')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, f'stardist_result_{i}.png'))\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "-ENSm5uWCi3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Execution"
      ],
      "metadata": {
        "id": "-Wpn9mVYClyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    train_image_dir = 'NucleiSegmentationDataset/all_images'\n",
        "    train_mask_dir = 'NucleiSegmentationDataset/merged_masks'\n",
        "    test_image_dir = 'TestDataset/images'\n",
        "    test_mask_dir = 'TestDataset/masks'\n",
        "\n",
        "    # Create the full training dataset\n",
        "    full_train_dataset = NucleiDatasetStardist(train_image_dir, train_mask_dir, image_size=(256, 256))\n",
        "\n",
        "    # Calculate sizes for training and validation sets\n",
        "    train_size = int(0.8 * len(full_train_dataset))\n",
        "    val_size = len(full_train_dataset) - train_size\n",
        "\n",
        "    # Split the training dataset into training and validation sets\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Create the test dataset\n",
        "    test_dataset = NucleiDatasetStardist(test_image_dir, test_mask_dir, image_size=(256, 256))\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # --- INSPECTION ---\n",
        "    print(\"\\n--- Data Inspection ---\")\n",
        "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "    print(f\"Testing dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    if len(train_dataset) > 0:\n",
        "        first_train_img, first_train_mask = train_dataset[0]\n",
        "        print(f\"First training image shape: {first_train_img.shape}, min: {first_train_img.min():.4f}, max: {first_train_img.max():.4f}\")\n",
        "        print(f\"First training mask shape: {first_train_mask.shape}, unique values: {np.unique(first_train_mask)}\")\n",
        "\n",
        "    if len(test_dataset) > 0:\n",
        "        first_test_img, first_test_mask = test_dataset[0]\n",
        "        print(f\"First testing image shape: {first_test_img.shape}, min: {first_test_img.min():.4f}, max: {first_test_img.max():.4f}\")\n",
        "        print(f\"First testing mask shape: {first_test_mask.shape}, unique values: {np.unique(first_test_mask)}\")\n",
        "    print(\"--- End Data Inspection ---\\n\")\n",
        "    # --- END INSPECTION ---\n",
        "\n",
        "    # Train StarDist model - Increased epochs, consider adjusting learning_rate if needed\n",
        "    stardist_model = train_stardist(train_loader, val_loader, epochs=15, learning_rate=1e-4) # Increased epochs to 50\n",
        "\n",
        "    # Predict on the test set - Adjusted prob_threshold and nms_threshold\n",
        "    test_images = np.array([item[0] for item in test_dataset])\n",
        "    test_ground_truth_masks = np.array([item[1] for item in test_dataset])\n",
        "    # Try different thresholds if predictions are still black\n",
        "    stardist_predictions = predict_stardist(stardist_model, test_loader, prob_threshold=0.1, nms_threshold=0.3)\n",
        "\n",
        "    # --- INSPECTION OF PREDICTIONS ---\n",
        "    print(\"\\n--- Prediction Inspection ---\")\n",
        "    if len(stardist_predictions) > 0:\n",
        "        print(f\"Shape of first prediction: {stardist_predictions[0].shape}\")\n",
        "        print(f\"Unique values in first prediction: {np.unique(stardist_predictions[0])}\")\n",
        "        if np.max(stardist_predictions[0]) == 0:\n",
        "            print(\"WARNING: First prediction is entirely black (no objects detected).\")\n",
        "    else:\n",
        "        print(\"No predictions generated.\")\n",
        "    print(\"--- End Prediction Inspection ---\\n\")\n",
        "    # --- END INSPECTION ---\n",
        "\n",
        "\n",
        "    # Evaluate (using mean IoU for instance segmentation)\n",
        "    mean_iou, precision, recall, f1, accuracy = evaluate_stardist(test_ground_truth_masks, stardist_predictions, iou_threshold=0.5)\n",
        "    print(f'Mean IoU on test set: {mean_iou:.4f}')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_stardist_results(test_images, test_ground_truth_masks, stardist_predictions, num_samples=5)\n",
        "\n",
        "    # Optionally save the trained model - Corrected file extension\n",
        "    model_save_path = 'stardist_nuclei_model.keras' # Changed extension to .keras (recommended by Keras)\n",
        "    stardist_model.keras_model.save(model_save_path)\n",
        "    print(f'Trained StarDist model saved to: {model_save_path}')"
      ],
      "metadata": {
        "id": "u2XlUwXWCnIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}