{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2407101,"sourceType":"datasetVersion","datasetId":1454704},{"sourceId":11918784,"sourceType":"datasetVersion","datasetId":7492953}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install scikit-image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:02:01.410594Z","iopub.execute_input":"2025-05-23T17:02:01.410832Z","iopub.status.idle":"2025-05-23T17:02:05.716675Z","shell.execute_reply.started":"2025-05-23T17:02:01.410807Z","shell.execute_reply":"2025-05-23T17:02:05.715922Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.26.4)\nRequirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.2)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->scikit-image) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->scikit-image) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->scikit-image) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->scikit-image) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24->scikit-image) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24->scikit-image) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io, color\nfrom skimage.transform import resize\nimport os\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F","metadata":{"id":"td7RbJqHB6Uh","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:47:02.594482Z","iopub.execute_input":"2025-05-23T17:47:02.595192Z","iopub.status.idle":"2025-05-23T17:47:07.296811Z","shell.execute_reply.started":"2025-05-23T17:47:02.595140Z","shell.execute_reply":"2025-05-23T17:47:07.296154Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Hybrid Attention Module (HAM)","metadata":{"id":"tD0-yw4TCk3-"}},{"cell_type":"code","source":"class HybridAttention(nn.Module):\n    def __init__(self, in_channels):\n        super(HybridAttention, self).__init__()\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // 8, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // 8, in_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n        self.spatial_attention = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // 8, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // 8, 1, kernel_size=3, padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        channel_attn = self.channel_attention(x)\n        spatial_attn = self.spatial_attention(x)\n        channel_refined = x * channel_attn\n        spatial_refined = x * spatial_attn\n        # Novel combination: Element-wise multiplication of channel and spatial refined features\n        attention_out = channel_refined * spatial_refined\n        return x + attention_out\n\nclass InceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels_1x1, red_channels_3x3, out_channels_3x3, red_channels_5x5, out_channels_5x5, pool_proj):\n        super(InceptionModule, self).__init__()\n        self.branch1 = nn.Conv2d(in_channels, out_channels_1x1, kernel_size=1)\n        self.branch2_red = nn.Conv2d(in_channels, red_channels_3x3, kernel_size=1)\n        self.branch2_conv = nn.Conv2d(red_channels_3x3, out_channels_3x3, kernel_size=3, padding=1)\n        self.branch3_red = nn.Conv2d(in_channels, red_channels_5x5, kernel_size=1)\n        self.branch3_conv = nn.Conv2d(red_channels_5x5, out_channels_5x5, kernel_size=5, padding=2)\n        self.branch4_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.branch4_proj = nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n\n    def forward(self, x):\n        branch1_out = F.relu(self.branch1(x))\n        branch2_out = F.relu(self.branch2_conv(F.relu(self.branch2_red(x))))\n        branch3_out = F.relu(self.branch3_conv(F.relu(self.branch3_red(x))))\n        branch4_out = F.relu(self.branch4_proj(self.branch4_pool(x)))\n        outputs = [branch1_out, branch2_out, branch3_out, branch4_out]\n        return torch.cat(outputs, 1)\n\nclass HMSAModule(nn.Module):\n    def __init__(self, in_channels, out_channels_1x1, red_channels_3x3, out_channels_3x3, red_channels_5x5, out_channels_5x5, pool_proj):\n        super(HMSAModule, self).__init__()\n        self.inception = InceptionModule(in_channels, out_channels_1x1, red_channels_3x3, out_channels_3x3, red_channels_5x5, out_channels_5x5, pool_proj)\n        inception_output_channels = out_channels_1x1 + out_channels_3x3 + out_channels_5x5 + pool_proj\n        self.attention = HybridAttention(inception_output_channels)\n        if inception_output_channels != in_channels:\n            self.projection = nn.Conv2d(inception_output_channels, in_channels, kernel_size=1)\n        else:\n            self.projection = None\n\n    def forward(self, x):\n        inception_out = self.inception(x)\n        attention_out = self.attention(inception_out)\n        if self.projection:\n            attention_out = self.projection(attention_out)\n        hmsam_output = x + attention_out\n        return hmsam_output","metadata":{"id":"YJea7sOmCoSk","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:47:09.381377Z","iopub.execute_input":"2025-05-23T17:47:09.381784Z","iopub.status.idle":"2025-05-23T17:47:09.392781Z","shell.execute_reply.started":"2025-05-23T17:47:09.381761Z","shell.execute_reply":"2025-05-23T17:47:09.392195Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### HMSAM-UNet Model","metadata":{"id":"40DGtAzoCvE2"}},{"cell_type":"code","source":"class HMSAM_UNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super(HMSAM_UNet, self).__init__()\n        def double_conv(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True)\n            )\n\n        self.enc1 = double_conv(in_channels, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = double_conv(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = double_conv(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = double_conv(256, 512)\n        self.pool4 = nn.MaxPool2d(2)\n\n        self.hmsam = HMSAModule(in_channels=512,\n                                out_channels_1x1=128,\n                                red_channels_3x3=96, out_channels_3x3=192,\n                                red_channels_5x5=32, out_channels_5x5=64,\n                                pool_proj=128)\n\n        self.upconv4 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n        self.dec4 = double_conv(1024, 512)\n\n        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = double_conv(512, 256)\n\n        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = double_conv(256, 128)\n\n        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = double_conv(128, 64)\n\n        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(self.pool1(enc1))\n        enc3 = self.enc3(self.pool2(enc2))\n        enc4 = self.enc4(self.pool3(enc3))\n        bridge = self.pool4(enc4)\n\n        hmsam_out = self.hmsam(bridge)\n\n        dec4 = self.dec4(torch.cat([self.upconv4(hmsam_out), enc4], dim=1))\n        dec3 = self.dec3(torch.cat([self.upconv3(dec4), enc3], dim=1))\n        dec2 = self.dec2(torch.cat([self.upconv2(dec3), enc2], dim=1))\n        dec1 = self.dec1(torch.cat([self.upconv1(dec2), enc1], dim=1))\n\n        out = torch.sigmoid(self.outc(dec1))\n        return out","metadata":{"id":"hgUzlAcHCyOb","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:47:09.788324Z","iopub.execute_input":"2025-05-23T17:47:09.788599Z","iopub.status.idle":"2025-05-23T17:47:09.798132Z","shell.execute_reply.started":"2025-05-23T17:47:09.788579Z","shell.execute_reply":"2025-05-23T17:47:09.797455Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Data Loading and Processing (Adapt for your dataset)","metadata":{"id":"Njue4XF8C4-w"}},{"cell_type":"code","source":"class NucleiDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_size=(128, 128)):\n        self.image_size = image_size\n        self.image_paths = []\n        self.mask_paths = []\n        image_files = sorted(os.listdir(image_dir))\n        mask_files = sorted(os.listdir(mask_dir))\n\n        for img_file, mask_file in zip(image_files, mask_files):\n            img_path = os.path.join(image_dir, img_file)\n            mask_path = os.path.join(mask_dir, mask_file)\n            self.image_paths.append(img_path)\n            self.mask_paths.append(mask_path)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n\n        img = io.imread(img_path)\n        mask = io.imread(mask_path)\n\n        if img.ndim == 3 and img.shape[2] == 4:\n            img = img[:, :, :3]\n\n        if mask.ndim == 3:\n            if mask.shape[2] == 4:\n                mask = mask[:, :, :3]\n            mask = color.rgb2gray(mask)\n        elif mask.ndim == 2:\n            pass\n\n        img_resized = resize(img, self.image_size, anti_aliasing=True)\n        if img_resized.ndim == 3:\n            img_gray = color.rgb2gray(img_resized)\n        else:\n            img_gray = img_resized\n\n        mask_resized = resize(mask, self.image_size, anti_aliasing=True)\n        mask_binary = mask_resized > 0.5\n\n        # CRUCIAL CHANGE: Resize the original images and masks too!\n        img_resized_orig = resize(img, self.image_size, anti_aliasing=True)\n        mask_resized_orig = resize(mask, self.image_size, anti_aliasing=True)\n\n\n        return torch.tensor(np.expand_dims(img_gray.astype(np.float32), axis=0)), \\\n               torch.tensor(np.expand_dims(mask_binary.astype(np.float32), axis=0)), \\\n               img_resized_orig, mask_resized_orig","metadata":{"id":"6SGvxjx5C8VK","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:47:10.132736Z","iopub.execute_input":"2025-05-23T17:47:10.133287Z","iopub.status.idle":"2025-05-23T17:47:10.141042Z","shell.execute_reply.started":"2025-05-23T17:47:10.133264Z","shell.execute_reply":"2025-05-23T17:47:10.140325Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Training","metadata":{"id":"ykBzC_QODCaO"}},{"cell_type":"code","source":"def train_model(model, train_loader, num_epochs=10, lr=1e-3):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for img_tensor, mask_tensor, _, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            img_tensor, mask_tensor = img_tensor.to(device), mask_tensor.to(device)\n            optimizer.zero_grad()\n            output = model(img_tensor)\n            loss = criterion(output, mask_tensor)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n    return model","metadata":{"id":"rfRNiyCMDEQC","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:47:10.720807Z","iopub.execute_input":"2025-05-23T17:47:10.721306Z","iopub.status.idle":"2025-05-23T17:47:10.726565Z","shell.execute_reply.started":"2025-05-23T17:47:10.721279Z","shell.execute_reply":"2025-05-23T17:47:10.725905Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Prediction","metadata":{"id":"zS3FKOE8DHqf"}},{"cell_type":"code","source":"def predict_model(model, test_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.eval()\n    preds = []\n    original_images = []\n    ground_truth_masks = []\n    with torch.no_grad():\n        for img_tensor, mask_tensor, original_img, original_mask in tqdm(test_loader, desc=\"Predicting\"):\n            img_tensor = img_tensor.to(device)\n            output = model(img_tensor).cpu().numpy()\n            preds.extend(output)\n            original_images.extend(original_img)\n            ground_truth_masks.extend(original_mask)\n    return np.array(preds), original_images, ground_truth_masks","metadata":{"id":"7qyIvVRDDLnn","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:47:11.239114Z","iopub.execute_input":"2025-05-23T17:47:11.239791Z","iopub.status.idle":"2025-05-23T17:47:11.244621Z","shell.execute_reply.started":"2025-05-23T17:47:11.239765Z","shell.execute_reply":"2025-05-23T17:47:11.243771Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Evaluation","metadata":{"id":"zm1tngWWDO2I"}},{"cell_type":"code","source":"def compute_metrics(y_true, y_pred, threshold=0.5):\n    y_pred_binary = y_pred > threshold\n    y_true_flat = y_true.flatten()\n    y_pred_flat = y_pred_binary.flatten()\n\n    TP = np.sum((y_true_flat == 1) & (y_pred_flat == 1))\n    TN = np.sum((y_true_flat == 0) & (y_pred_flat == 0))\n    FP = np.sum((y_true_flat == 0) & (y_pred_flat == 1))\n    FN = np.sum((y_true_flat == 1) & (y_pred_flat == 0))\n\n    precision = TP / (TP + FP + 1e-8)\n    recall = TP / (TP + FN + 1e-8)\n    accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-8)\n    f1_score = 2 * precision * recall / (precision + recall + 1e-8)\n\n    return precision, recall, f1_score, accuracy","metadata":{"id":"SyMlz92PDR1x","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:47:11.981756Z","iopub.execute_input":"2025-05-23T17:47:11.982005Z","iopub.status.idle":"2025-05-23T17:47:11.987104Z","shell.execute_reply.started":"2025-05-23T17:47:11.981987Z","shell.execute_reply":"2025-05-23T17:47:11.986461Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Visualization","metadata":{"id":"Apwy7BoJDUlh"}},{"cell_type":"code","source":"def visualize_results(original_images, ground_truth_masks, predictions, num_samples=5, save_dir='results/hmsam_unet'):\n    os.makedirs(save_dir, exist_ok=True)\n    for i in range(min(num_samples, len(original_images))):\n        plt.figure(figsize=(15, 5))\n\n        # Original image\n        plt.subplot(1, 3, 1)\n        if original_images[i].ndim == 3:\n            plt.imshow(original_images[i])\n        else:\n            plt.imshow(original_images[i], cmap='gray')\n        plt.title('Original Image')\n        plt.axis('off')\n\n        # Ground truth mask\n        plt.subplot(1, 3, 2)\n        if ground_truth_masks[i].ndim == 3:\n            plt.imshow(ground_truth_masks[i])\n        else:\n            plt.imshow(ground_truth_masks[i], cmap='gray')\n        plt.title('Ground Truth Mask')\n        plt.axis('off')\n\n        # Prediction (binarized output)\n        plt.subplot(1, 3, 3)\n        plt.imshow(predictions[i][0] > 0.5, cmap='gray')\n        plt.title('Prediction')\n        plt.axis('off')\n\n        plt.savefig(os.path.join(save_dir, f'prediction_result_{i}.png'))\n        plt.close()","metadata":{"id":"Zd9YOX6zDXse","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:47:12.768641Z","iopub.execute_input":"2025-05-23T17:47:12.769212Z","iopub.status.idle":"2025-05-23T17:47:12.775307Z","shell.execute_reply.started":"2025-05-23T17:47:12.769179Z","shell.execute_reply":"2025-05-23T17:47:12.774459Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Main Execution","metadata":{"id":"KKtv8InsDbhh"}},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Define your data directories\n    image_dir = '/kaggle/input/covid19-ct-scan-lesion-segmentation-dataset/frames'  # Replace with your image directory\n    mask_dir = '/kaggle/input/covid19-ct-scan-lesion-segmentation-dataset/masks'    # Replace with your mask directory\n\n    # Ensure the directories exist\n    if not os.path.exists(image_dir) or not os.path.exists(mask_dir):\n        print(f\"Error: Image directory '{image_dir}' or mask directory '{mask_dir}' not found. Please check the paths.\")\n    else:\n        dataset = NucleiDataset(image_dir, mask_dir, image_size=(128, 128))\n\n        if len(dataset) == 0:\n            print(f\"Error: No images found in '{image_dir}' or masks in '{mask_dir}'. Please ensure files are present.\")\n        else:\n            train_size = int(0.8 * len(dataset))\n            test_size = len(dataset) - train_size\n\n            if train_size == 0:\n                print(\"Error: Not enough data to create a training set. Please provide more images.\")\n            elif test_size == 0:\n                print(\"Error: Not enough data to create a test set. Please provide more images.\")\n            else:\n                train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n                # Use shuffle=True for training loader for better generalization\n                train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n                test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n                # Instantiate the HMSAM-UNet model\n                hmsam_unet_model = HMSAM_UNet(in_channels=1, out_channels=1) # Assuming grayscale input\n\n                # Train the model\n                print(\"Starting model training...\")\n                hmsam_unet_model = train_model(hmsam_unet_model, train_loader, num_epochs=10, lr=0.001)\n                print(\"Model training complete.\")\n\n                # Get predictions and original images for evaluation and visualization\n                print(\"Starting prediction on test set...\")\n                predictions, original_test_images, original_test_masks = predict_model(hmsam_unet_model, test_loader)\n                print(\"Prediction complete.\")\n\n                # Convert original masks to binary for metric calculation\n                binary_original_test_masks = [resize(mask, (128, 128), anti_aliasing=True) > 0.5 for mask in original_test_masks]\n                binary_original_test_masks_np = np.array([np.expand_dims(mask.astype(np.float32), axis=0) for mask in binary_original_test_masks])\n\n                # Compute evaluation metrics\n                precision, recall, f1, accuracy = compute_metrics(binary_original_test_masks_np, predictions, threshold=0.5)\n                print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, Accuracy: {accuracy:.4f}')\n\n                # Visualize the results\n                print(\"Saving visualization results...\")\n                visualize_results(original_test_images, original_test_masks, predictions, num_samples=5)\n                print(\"Visualization complete. Check 'results/hmsam_unet' directory.\")","metadata":{"id":"LMFV9LyXDfiD","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T17:55:06.897316Z","iopub.execute_input":"2025-05-23T17:55:06.897588Z","iopub.status.idle":"2025-05-23T18:23:55.898952Z","shell.execute_reply.started":"2025-05-23T17:55:06.897567Z","shell.execute_reply":"2025-05-23T18:23:55.898238Z"}},"outputs":[{"name":"stdout","text":"Starting model training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 219/219 [02:54<00:00,  1.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 1.4690\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 219/219 [02:49<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 1.4797\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 219/219 [02:49<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Loss: 1.4822\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 219/219 [02:48<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Loss: 1.4846\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 219/219 [02:48<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Loss: 1.4804\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 219/219 [02:48<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Loss: 1.4824\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 219/219 [02:48<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Loss: 1.4829\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 219/219 [02:45<00:00,  1.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Loss: 1.4811\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 219/219 [02:47<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Loss: 1.4831\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 219/219 [02:45<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Loss: 1.4833\nModel training complete.\nStarting prediction on test set...\n","output_type":"stream"},{"name":"stderr","text":"Predicting: 100%|██████████| 546/546 [00:43<00:00, 12.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Prediction complete.\nPrecision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Accuracy: 0.9847\nSaving visualization results...\nVisualization complete. Check 'results/hmsam_unet' directory.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}